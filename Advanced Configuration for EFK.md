![fwo2rbhfzkemfbzg7mzl](https://github.com/user-attachments/assets/70b6315b-939b-4607-8dae-24f7015d14cc)üìú Advanced Configurations for Optimizing the EFK Stack on Kubernetes
In this guide, we will explore how to enhance the efficiency and functionality of the Elasticsearch, Fluentd, and Kibana (EFK) stack on Kubernetes. This includes diving into custom Fluentd plugins for log enrichment and Elasticsearch index management strategies for performance optimization. Our goal is to provide you with a comprehensive understanding and practical skills to improve your EFK stack.

üïÆ Introduction to the EFK Stack
The EFK stack combines Elasticsearch, Fluentd, and Kibana to efficiently manage, search, and visualize logs generated by applications running in Kubernetes clusters.

Elasticsearch acts as a highly scalable search and analytics engine.
Fluentd serves as an open-source data collector for unified logging, allowing you to unify data collection and consumption for better use and understanding of data.
Kibana provides the visualization layer, offering powerful and user-friendly interfaces to view and analyze the data stored in Elasticsearch.
‚úç Architecture Diagram

![fwo2rbhfzkemfbzg7mzl](https://github.com/user-attachments/assets/d1b28f39-e6ff-4969-98ab-65f5bbaef760)

üîñ Custom Fluentd Plugins for Log Enrichment
Log enrichment involves adding additional context to your logs, making them more informative and easier to analyze. Custom Fluentd plugins can be developed or configured to enrich logs with extra metadata, such as Kubernetes labels, environment information, or application-specific data.

Creating a Custom Fluentd Filter Plugin
Set up your Fluentd environment: Ensure you have Fluentd installed and running in your Kubernetes cluster. You can use the Fluentd daemonset for easy deployment.

Develop the plugin: Fluentd plugins are typically written in Ruby. Here's a simple example of a custom filter plugin that adds a static field to all logs:

```
require 'fluent/plugin/filter'

module Fluent::Plugin
  class MyCustomFilter < Filter
    Fluent::Plugin.register_filter('my_custom_filter', self)

    def configure(conf)
      super
      # You can add configuration parameters here
    end

    def filter(tag, time, record)
      # Add a custom field to the record
      record["additional_info"] = "static_value"
      record
    end
  end
end
```

Install the plugin: After developing your plugin, you need to make it available to Fluentd. If you package it as a gem, you can install it using Fluentd's fluent-gem command.

Configure Fluentd to use the plugin: Modify your Fluentd configuration to use the new filter. Here's an example configuration snippet:


```
<filter **>
  @type my_custom_filter
</filter>
```

The plugin that you see above is the filter plugin. It is used to modify the structure of a log message. We are using it here in conjunction with our custom plugin that adds a new field to the logs.
Another important plugin that Fluentd uses is the buffer plugin which temporarily stores logs before forwarding them to the destination. It, thereby, helps in avoiding data loss.

Testing and Debugging
Ensure to test your plugin thoroughly in a development environment before deploying it to production. Fluentd provides detailed logs that can help you debug issues with your custom plugins.

üß∞ Elasticsearch Index Management for Performance
Proper index management is crucial for maintaining the performance of your Elasticsearch cluster. This involves strategies such as index rollover, sharding, and replicas configuration.

Index Rollover
Index rollover helps in managing indices based on certain criteria like size, age, or document count. It allows you to automate the creation of new indices when the current ones meet specified conditions.

Create an index template:

```
PUT _template/my_logs_template
{
  "index_patterns": ["logs-*"],
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 1
  }
}
```

Set up an ILM (Index Lifecycle Management) policy:
```
PUT _ilm/policy/my_logs_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "5GB",
            "max_age": "30d"
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

Apply the ILM policy to your index template:
```
PUT _template/my_logs_template
{
  "settings": {
    "index.lifecycle.name": "my_logs_policy"
  }
}
```

Sharding and Replicas Configuration
Properly configuring shards and replicas can significantly impact your Elasticsearch cluster's performance and resilience.

Shards: Distribute data across multiple nodes in the cluster to improve performance. The minimum value of master-eligible ELasticsearch nodes in a cluster is 3 to ensure high availability. The optimal number of shards depends on the size of your data and the capacity of your nodes.
Replicas: Provide high availability and redundancy. In case of a node failure, replicas ensure that your data is not lost.

```
PUT /my_logs-000001
{
  "settings": {
    "index": {
      "number_of_shards": 3, 
      "number_of_replicas": 2 
    }
  }
}
```

Apart from the above configurations, changing the refresh interval for an Elasticsearch index is another vital setting for improving write performance in an environment with intense log load.

üéâ Conclusion
Optimizing the EFK stack on Kubernetes involves a combination of custom log enrichment and efficient Elasticsearch index management. 
By developing custom Fluentd plugins, you can enhance the quality and usefulness of your logs. Meanwhile, proper index management strategies like rollover, 
sharding, and replicas configuration help maintain Elasticsearch performance at scale. With these advanced configurations, you can significantly improve your 
logging infrastructure's efficiency and reliability.



-----------------------------------------------------------------------------------------------------------
# Tasks

We have just discussed the importance of custom Fluentd plugins for log enrichment. Now, create a custom Fluentd filter plugin named my_custom_filter that adds a field called additional_data with the value static_info to all processed logs.

Place this file in the /root/fluentd/plugins folder. Create the directory not present in the above path, if any.

You just need to create the filter file for this task.
Note that fluentd accepts the files starting with prefix filter_ as plugins, so name the file accordingly.


Remember to follow the Ruby plugin development guide for Fluentd.

Access the Kibana UI from the top right of this page. Create an index pattern and analyze the logs.

Solution:
Create a new directory plugins inside the /root/fluentd/ folder. We will place our custom plugin file here.

Create a file named filter_my_custom_filter.rb in this directory with the following contents:

```
require 'fluent/plugin/filter'

class Fluent::Plugin::MyCustomFilter < Fluent::Plugin::Filter
  Fluent::Plugin.register_filter('my_custom_filter', self)

  config_param :additional_data, :string, default: 'static_info'

  def filter(tag, time, record)
    record['additional_data'] = @additional_data
    record
  end
end
```


---------------------------------------------------
Tasks

Having already created the custom plugin file, let's now add it to our fluentd configuration file in the /root/fluentd/etc folder.

Use the filter tag to add a new section to the config file including the custom plugin.

Ensure that this section is added before the match section.


Refer to the Description page to view how to add the plugin in the configuration file.


Solution
To the fluent.conf file inside the /root/fluentd/etc/ directory, add the following before the match section:
```
<filter **>
  @type my_custom_filter
  additional_data static_info
</filter>
```

The typical order of sections in a Fluentd configuration file is as follows:

Input plugins (<source> section)
<filter> section (optional, for filtering and modifying logs)
Output plugins (<match> section)
By placing the <filter> section before the <match> section, we ensure that logs are processed and filtered as needed before being sent to the output destinations specified in the <match> section.

Save the file and exit.

--------------------------------------------------------------------

task
The typical order of sections in a Fluentd configuration file is as follows:

Input plugins (<source> section)
<filter> section (optional, for filtering and modifying logs)
Output plugins (<match> section)
By placing the <filter> section before the <match> section, we ensure that logs are processed and filtered as needed before being sent to the output destinations specified in the <match> section.

Save the file and exit.


Solution:
First, delete the existing daemonset configuration by using this command:
```
kubectl delete ds fluentd
```
Append the following to the volumes section of fluentd.yaml file:

```
- name: pluginpath
  hostPath:
    path: /root/fluentd/plugins
```

and the following to the volumeMounts section:
```
- name: pluginpath
  mountPath: /fluentd/plugins
```

The final fluentd.yaml file will look like this:
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: elastic-stack
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.14.1-debian-elasticsearch7-1.0
        env:
          - name:  FLUENT_ELASTICSEARCH_HOST
            value: "elasticsearch.elastic-stack.svc.cluster.local"
          - name:  FLUENT_ELASTICSEARCH_PORT
            value: "9200"
          - name: FLUENT_ELASTICSEARCH_SCHEME
            value: "http"
          - name: FLUENTD_SYSTEMD_CONF
            value: disable
          - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
            value: /var/log/containers/fluent*
          - name: FLUENT_ELASTICSEARCH_SSL_VERIFY
            value: "false"
          - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE
            value: /^(?<time>.+) (?<stream>stdout|stderr)( (?<logtag>.))? (?<log>.*)$/ 
          - name:  FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
            value: "fluentd"
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: configpath
          mountPath: /fluentd/etc
        - name: pluginpath
          mountPath: /fluentd/plugins
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: configpath
        hostPath:
          path: /root/fluentd/etc
      - name: pluginpath
        hostPath:
          path: /root/fluentd/plugins
```

Use the following command to update the daemonset definition:
```
kubectl apply -f fluentd.yaml
```


------------------------------------------------
Task
Configure Elasticsearch index lifecycle management by creating an ILM policy named my_logs_policy that specifies a rollover action when the index reaches 5GB or 30 days old, and a delete action after 90 days.

Use the Elasticsearch API to create this ILM policy.


Note: Elasticsearch is accessible at localhost:30200.

Refer to the Description page to view the general structure of an ILM policy.

Solution


Navigate to the elastic-search folder and create a file titled my_logs_policy.json with the following contents:
```
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "5GB",
            "max_age": "30d"
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

To apply this policy, make use of the Elasticsearch API:
```
curl -X PUT "http://localhost:30200/_ilm/policy/my_logs_policy" -H 'Content-Type: application/json' -d @my_logs_policy.json
```
To verify the creation of your ILM policy, you can either use the GET request to the elasticsearch API:

```
curl -X GET "http://localhost:30200/_ilm/policy"
```

OR
Navigate to the Management menu present on the left sidebar of the Kibana UI. Click on Index Lifecycle Policies and you should see a policy titled my_logs_policy there.

OR
Navigate to Dev Tools -> Console and query the following:
GET _ilm/policy/my_logs_policy


-------------------------------------------------------------------------------

Task
As a final step, we will create an Index Template so that we can apply the index policy my_logs_policy to it.


The name of the template should be my_logs_template, and it should reference the index patterns starting with fluentd-*.

Provide a rollover_alias my-logs to this template.

Solution:

Navigate to the elastic-search folder and create a JSON document titled my_logs_template.json with the following contents:
```
{
  "index_patterns": ["fluentd-*"],
  "settings": {
    "index.lifecycle.name": "my_logs_policy",
    "index.lifecycle.rollover_alias": "my-logs"
  }
}
```

Apply the template to Elasticsearch using the following command:
```
curl -X PUT "http://localhost:30200/_template/my_logs_template" -H 'Content-Type: application/json' -d @my_logs_template.json
```

To verify this template, either run the following command:
```
curl -X GET "http://localhost:30200/_template"
```


OR

On the Kibana UI, navigate to Dev Tools-> Console and run the following query:
GET _cat/templates

You will see a list of all templates also including my_logs_template.
